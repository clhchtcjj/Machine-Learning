# PCA

主成分分析(Principe Component Analysis，PCA)，属于最常用的降维方法之一。顾名思义，主成分分析就是找出数据的主要部分来替代表示原始数据。

所谓主要部分，就是用它来表示原始数据时，误差小。

那么如何衡量这个误差呢？

首先，将数据从高维降低到低维，实际上就是找一个超平面，使其能够对所有样本进行恰当的表达。

那么，PCA主要从两个角度来衡量：

- 最近重构性：样本点到这个超平面的距离都足够接近
- 最大可分性：样本点在这个超平面上的投影能尽可能分开



## 1. 算法流程

**输入：**n维样本集$D=(x^{(1)}, x^{(2)},...,x^{(m)})$

**输出：**降维后的样本集$D'$，维度为$d'$

（1） 对所有样本进行去中心化：$x^{(i)} = x^{(i)} - \frac{1}{m}\sum\limits_{j=1}^{m} x^{(j)}$，即$\sum_ix^{(i)} = 0$

（2）计算样本的协方差矩阵：$XX^T$

（3）对协方差矩阵做特征值分解

（4）取最大的$d'$个特征值所对应的特征向量$w_1,w_2,...,w_{d'}$，将所有的特征向量标准化后，组成特征向量矩阵$W$

​	或者按阈值$t$取：

​									$$\frac{\sum\limits_{i=1}^{n'}\lambda_i}{\sum\limits_{i=1}^{n}\lambda_i} \geq t $$

（5）对样本集中的每一个样本$x^{(i)}$,转化为新的样本$z^{(i)}=W^Tx^{(i)}$

（6）得到输出样本集$D' =(z^{(1)}, z^{(2)},...,z^{(m)})$



## 2. 问题定义与符号说明

首先，假设数据样本$D=({\bf{x}}^{(1)}, {\bf{x}}^{(2)},...,{\bf{x}}^{(m)})$进行过中心化，即$\sum_i{\bf{x}}^{(i)} = \bf{0}$；经过投影变换后的新坐标系为$\{{\bf{w}}_1,{\bf{w}}_2,...,{\bf{w}}_n\}$，其中$w_i$均为标准正交基，即$||{\bf{w}}_i||_2=1, {\bf{w}}_i^T{\bf{w}}_j=0，(i\neq j)$。

假设，原始数据的维度是$d$，那么$ {\bf{W}}：n\times n$、${\bf{z}_i}：d'\times 1$、${\bf{x}}^{(i)} : n\times 1$、${\bf{\hat{x}}}^{(i)}：d'\times 1 $

明确几个简单概念：

- 正交基：$\{{\bf{w}}_1,{\bf{w}}_2,...,{\bf{w}}_n\}$

- 原始数据：${\bf{x}}^{(i)} = \sum_j^nz_{ij}{\bf{w}}_j$ => ${\bf{x}}^{(i)}= {\bf{W}}^T{\bf{z}}_i$

- 基坐标/投影：$z_{ij}={\bf{w}}_j^T{\bf{x}}^{(i)}$ =>  ${\bf{z}_i} = {\bf{W}}^T{\bf{x}}^{(i)}$

- 方差：$\frac{1}{m}\sum_{i=1}^m||{\bf{W}}^T{\bf{x}}^{(i)}||^2_2=\frac{1}{m}\sum_{i=1}^m({\bf{W}}^T{\bf{x}}^{(i)})^T({\bf{W}}^T{\bf{x}}^{(i)})=\frac{1}{m}\sum_{i=1}^mtr(({\bf{W}}^T{\bf{x}}^{(i)})({\bf{W}}^T{\bf{x}}^{(i)})^T)=tr({\bf{W}}^TS{\bf{W}})$

  ​		$S=\frac{1}{m}\sum_i{\bf{x}}^{(i)}{\bf{x}}^{(i)T}=\frac{1}{m}{\bf{X}}{\bf{X}}^T$

- 降维重建后的数据为：${\bf{\hat{x}}}^{(i)} = \sum_j^{d'}z_{ij}{\bf{w}}_j$

- $\sum_i{\bf{x}}_i{\bf{x}}_i^T={\bf{X}}{\bf{X}}^T$



## 3. PCA的推导与解释

## 3.1 最近重构性

![](https://raw.githubusercontent.com/clhchtcjj/Pit-for-Typora/master/PCA-1.jpg)

## 3.2 最大可分性

![](https://raw.githubusercontent.com/clhchtcjj/Pit-for-Typora/master/PCA-2.jpg)

## 3.3 降维 

![](https://raw.githubusercontent.com/clhchtcjj/Pit-for-Typora/master/PCA-3.jpg)



## 4. KPCA

PCA算法中存在一个假设：存在一个线性的超平面，可以对数据进行投影。

那当数据不是线性可分的时候，便不能直接进行PCA降维。

核主成分分析（kernel PCA）可以用于对非线性数据进行降维。它借鉴了和SVM一样的核函数的思想：先把数据从$n$维映射到线性可分的更高维空间$N$，然后从$N$维降低到低维$d'$。其中，$d'<n<N$。

则对于n维空间的特征分解：

​				$$\sum\limits_{i=1}^{m}x^{(i)}x^{(i)T}W=\lambda W$$

映射为：

​				$$\sum\limits_{i=1}^{m}\phi({\bf{x}}^{(i)})\phi({\bf{x}}^{(i)})^TW=\lambda W$$

通过在高维空间进行协方差矩阵的特征值分解，然后用和PCA一样的方法进行降维。一般来说，映射$\phi$不用显式的计算，而是在需要计算的时候通过核函数完成。由于KPCA需要核函数的运算，因此它的计算量要比PCA大很多。



## 5. PCA算法总结

- 算法类型：非监督
- 应用场景：应用最为广泛的是人脸识别。总的来说，PCA是一种粗糙的降维方法，对于维度不是很高的小数据集来说，效果可能不如因子分解法，但是当数据维度较高时，PCA效果不错。
- 不适用的场景：PCA是解除数据间的线性相关，但是当数据中存在高阶的线性相关性时，PCA无能为力，这时候可以考虑KPCA
- 优点：
  - 仅需要计算数据协方差矩阵的特征值，计算简单，易于实现
  - 可以消除原始数据成分之间相互影响的因素，便于数据处理与分析
- 缺点：
  - 不具有解释性
  - 丢弃的方差小的非主成分（参考3.3）可能含有对样本差异的重要信息，丢弃后可能影响后面的数据分析



## 参考文献

【1】 周志华 《机器学习》

【2】 刘建平 [主成分分析（PCA）原理总结](https://github.com/clhchtcjj/Pit-for-Typora/blob/master/PCA-1.jpg)