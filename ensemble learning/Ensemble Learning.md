---
layout: post
title: 集成学习
categories: machine learning
author: CC
tags: machine learning

---

# 集成学习（Ensemble Learning）



> 集成学习是现今运用比较广泛的一种机器学习方法，它是由多个较弱的模型集成模型组，其中模型可以单独进行训练，并且他们的预测能以某种方式结合起来去做一个总体预测。说白了，有点“三个臭皮匠赛过诸葛亮”和“博采众长”的意思。下面就对集成学习的相关知识做一个汇总。

## 集成学习概述

集成学习通过将多个个体学习器进行结合，常可获得比单一学习器显著优越的泛化性能。一般来说，实现一个集成学习需要解决如下两个问题：

- 如何得到若干个体学习器？
- 如何选择一种结合策略，将这些个体学习器集合成一个强学习器？

首先来回答第一个问题。通常有两种途径来选择个体学习器：

- 同质学习器

  所谓同质的就是所有个体学习器属于一个种类，例如都是决策树，都是BP神经网络。

- 异质学习器：

  所谓异质的就是所有个体学习器不都属于一个种类，例如集成逻辑回归分类器和支持向量机分类器解决某一分类问题。

相对来说，同质集成应用比较广泛。

同质个体学习器按照个体学习器之间是否存在依赖关系可以分为两类：（1）存在强依赖关系的个体学习器需要串行生成，代表算法是boosting系列算法；（2）不存在依赖关系的个体学习器可以并行生成，代表算法是bagging系列算法。

## boosting集成（串行）

boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习器的学习错误率表现来更新训练样本的权重，使得之前弱学习器1学习错误率高的样本点的权重变高，使得这些样本点在下一轮弱学习器2的训练中得到更多的重视。如此重复进行，直到弱学习器的数量达到事先指定的数目T，最终将T个弱学习器按照一定的集合策略进行整合，以期得到最终的强学习器。

**代表算法**

- AdaBoost
- 提升树（boosting tree）

**boosting与重采样**

从boosting的工作机制可以看出，boosting算法要求个体学习器能够对特定的数据分布进行学习，即在训练过程的每一轮中，根据样本分布为每个训练样本重新赋予一个权重。对于无法接受带权样本的个体学习器可以通过“重采样”法来处理，即在每一轮训练中，根据样本分布对训练集进行重采样，再用重采样得到的样本集训练新的个体学习器。

一般而言，“重赋权”与“重采样”没有太大的差别，但是boosting算法在训练的每一轮都要判断当前的个体学习器是否有效，一旦条件不满足，则当前个体学习器将被抛弃，算法停止。这就意味着，可能获得的个体学习器的个数达不到T，那么在模型将会因为个体学习器不够而效果不佳。"重采样"法可以避免这一问题，即在当前个体学习器不满足要求是被抛弃后，根据当前分布重新对样本进行重采样，再继续学习新的个体学习器，直至达到数量T。

**偏差与方差角度分析**

> **偏差**：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；
>
> **方差**：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；
>
> **噪声：** 表达了当前任务是上任何学习算法所能达到的期望泛化误差的下界，即刻画了问题本身的难度。
>
> 一个模型的泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。给定学习任务，为了取得好的泛化性能，则需要使偏差较小，即能充分拟合数据，并且是方差较小，即使得数据扰动产生的影响小。



## bagging集成（并行）

bagging的个体学习器的训练集是通过随机采样得到的。通过T次的随机采样，我们可以得到T个采样集，对于这T个采样集，我们可以分别独立的训练出T个弱学习器，再通过一定的集合策略进行整合，以期得到最终的强学习器。

对于这里的随机采样有必要做进一步的介绍，这里一般采用的是自助采样法（Bootstap sampling）,即对于m个样本的原始训练集，我们每次先随机采集一个样本放入采样集，接着把该样本放回，也就是说下次采样时该样本仍有可能被采集到，这样采集m次，最终可以得到m个样本的采样集，由于是随机采样，这样每次的采样集是和原始训练集不同的，和其他采样集也是不同的，这样得到多个不同的弱学习器。

**代表算法**

- 随机森林

**偏差与方差角度分析**

bagging算法主要关注降低方差，因为它在易受样本扰动的学习器上效用更为明显。

## Bagging，Boosting二者之间的区别
- 样本选择上：

Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

- 样例权重：

Bagging：使用均匀取样，每个样例的权重相等。
Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

- 预测函数：

Bagging：所有预测函数的权重相等。
Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

- 并行计算：

Bagging：各个预测函数可以并行生成。
Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

## 结合策略

我们假定我得到的T个弱学习器是${h_1,h_2,...h_T}$

### 平均法

对于数值类的**回归预测**问题，通常使用的结合策略是平均法，也就是说，对于若干和弱学习器的输出进行平均得到最终的预测输出。

最简单的平均是算术平均，也就是说最终预测是$$H(x) = \frac{1}{T}\sum\limits_{1}^{T}h_i(x)$$

如果每个个体学习器有一个权重$w$，则最终预测是$$H(x) = \sum\limits_{i=1}^{T}w_ih_i(x)$$

其中$w_i$是个体学习器$h_i$的权重，通常有$$w_i \geq 0 ,\;\;\; \sum\limits_{i=1}^{T}w_i = 1$$

一般来说，在个体学习器性能差异较大时使用加权平均法，而在个体学习器性能相近时使用简单平均法。

### 投票法

对于分类问题的预测，我们通常使用的是投票法。假设我们的预测类别是$\{c_1,c_2,...c_K\}$,对于任意一个预测样本x，我们的T个弱学习器的预测结果分别是$(h_1(x), h_2(x)...h_T(x))$。

最简单的投票法是相对多数投票法，也就是我们常说的少数服从多数，也就是T个弱学习器的对样本x的预测结果中，数量最多的类别$c_i$为最终的分类类别。如果不止一个类别获得最高票，则随机选择一个做最终类别。

稍微复杂的投票法是绝对多数投票法，也就是我们常说的要票过半数。在相对多数投票法的基础上，不光要求获得最高票，还要求票过半数。否则会拒绝预测。

更加复杂的是加权投票法，和加权平均法一样，每个弱学习器的分类票数要乘以一个权重，最终将各个类别的加权票数求和，最大的值对应的类别为最终类别

一般来说，基于**类概率**的投票比基于**类标记**的投票性能要好。

### 学习法

所谓“学习法”就是通过另一个学习器来进行结合，如stacking和BMA（贝叶斯模型平均）。理论上来说，当数据生成模型恰在当前考虑的模型中，且数据噪声很少，则BMA不差于stacking，否则stacking要不BMA更鲁棒。

## 总结

下面是将决策树与这些算法框架进行结合所得到的新的算法：

1. Bagging + 决策树 = 随机森林
2. AdaBoost + 决策树 = 提升树
3. Gradient Boosting + 决策树 = GBDT

bagging是减少variance，而boosting是减少bias。



