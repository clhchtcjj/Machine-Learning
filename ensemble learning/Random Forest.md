# Random Forest（随机森林）

> 　随机森林是集成学习中可以和梯度提升树GBDT分庭抗礼的算法，尤其是它可以很方便的并行训练，在如今大数据大样本的的时代很有诱惑力。

Bagging算法的核心在于随机采样（boostrap），即训从练集里面采集固定个数的样本，但是每采集一个样本后，都将样本放回。也就是说，之前采集到的样本在放回后有可能继续被采集到。

对于我们的Bagging算法，一般会随机采集和训练集样本数m一样个数的样本。这样得到的采样集和训练集样本的个数相同，但是样本内容不同。如果我们对有m个样本训练集做T次的随机采样，则由于随机性，T个采样集各不相同。

与GBDT子采样不同，GBDT的子采样是无放回的，而bagging是有放回的。

在bagging的每轮随机采样中，训练集中大约有$\frac{1}{e}=0.368$的数据没有被采样集采集中。这部分数据由于没有被采样到，可以用于检验模型的泛化能力。

## Bagging算法流程

**输入：**样本集$D=\{(x_,y_1),(x_2,y_2), ...(x_m,y_m)\}$，弱学习器算法, 弱分类器迭代次数T。

**输出：**最终的强分类器$f(x)$

(1) 对于t=1,2...,T

​	(a) 对训练集进行第t次随机采样，共采集m次，得到包含m个样本的采样集$D_m$

​	(b) 用采样集$D_m$训练第m个弱学习器$G_m(x)$

(2) 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。

## 随机森林算法

随机森林算法选择的个体学习器是CART决策树，并对它做了改进，即普通的决策树，会在节点上所有$n$个样本特征中选择一个最优特征来所决策树左右子树划分的点，但是RF是通过随机选节点上的一部分样本也想，这个数字小于$n$。，这样能从一定角度增强模型泛化能力。

**算法流程**

**输入：**样本集$D=\{(x_,y_1),(x_2,y_2), ...(x_m,y_m)\}$，弱学习器算法, 弱分类器迭代次数T。

**输出：**最终的强分类器$f(x)$

(1) 对于t=1,2...,T

​	(a) 对训练集进行第t次随机采样，共采集m次，得到包含m个样本的采样集$D_m$

​	(b) 用采样集$D_m$训练第m个弱学习器$G_m(x)$，在进行划分时，在节点上所有的样本特征中选择一部分样本特征，在这些随机选择的部分样本特征中选择一个最优的特征来做为划分依据。

(2) 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。

## 随机森林的推广

### extra trees

原理与流程与RF基本相同，存在两点区别：

（1）不采用bootstrap采样，每次训练都使用原始训练集

（2）随机选择一个特征进行划分，而不是像RF那样基于增益、基尼系数、均方差等

随机选择划分点，可以提高模型的泛化能力，但是会增大模型和模型的偏差

### Totally Random Trees Embedding

是一种非监督学习的数据转化方法：在数据转化的过程也使用了类似于RF的方法，建立T个决策树来拟合数据。当决策树建立完毕以后，数据集里的每个数据在T个决策树中叶子节点的位置也定下来了。比如我们有3颗决策树，每个决策树有5个叶子节点，某个数据特征xx划分到第一个决策树的第2个叶子节点，第二个决策树的第3个叶子节点，第三个决策树的第5个叶子节点。则x映射后的特征编码为(0,1,0,0,0,     0,0,1,0,0,     0,0,0,0,1), 有15维的高维特征。这里特征维度之间加上空格是为了强调三颗决策树各自的子编码。

映射到高维特征后，可以继续使用监督学习的各种分类回归算法了。

## 随机森林优缺点

**优点：**

- 训练具有高度并行化，适合大数据场景
- **训练后可以输出各个特征的重要程度**
- bootstrap的采样方法，能够降低模型的方差，增强模型的泛化能力
- 实现简单
- 对缺失的特征不敏感

**缺点：**

- 在噪音比较大的样本集上，RF容易陷入过拟合
- 取值划分比较多的特征容易对RF产生更大的影响，从而影响模型的拟合效果

## 参考文献

【1】刘建平 [Bagging与随机森林算法原理小结](http://www.cnblogs.com/pinard/p/6156009.html)







