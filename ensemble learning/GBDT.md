# GBDT算法

> GBDT是Boosting+决策树的算法，与Adaboost的不同之处在于：Adaboost是利用前一轮弱学习器的误差率来更新训练集的权重，这样一轮轮迭代下去。GBDT也是迭代的前向分步算法，但是弱学习器限定了只能使用CART回归树。

## 基本思想

概括来说，GBDT的思想就是在每一轮迭代中，让损失的损失尽可能小。假设我们前一轮迭代得到的强学习器是$f_{t-1}(x)$, 损失函数是$L(y,f_{t-1}(x))$, 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器$h_t(x)$，让本轮的损失损失$L(y,f_{t}(x) =L(y,f_{t-1}(x)+ h_t(x))$最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。

GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。

## 如何衡量损失

衡量损失最直接的方法是残差（提升树），当提升树的损失函数是平方损失或指数损失时，其优化过程是简单的。但对一般损失函数而言，往往每一步优化并不是那么容易的。针对这一问题，Freidman提出了梯度提升算法。这是利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值

​					$$r_{ti} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]_{f(x) = f_{t-1}(x)}$$

作为回归问题的提升树算法的残差的近似值，拟合一个回归树。



## 算法步骤

### 回归

**输入：**训练数据集$$T={(x_,y_1),(x_2,y_2), ...(x_m,y_m)}$$，其中$x_i \in R^n$，$y_i\in R$

**输出：**最终回归树$f(x)$

(1) 初始化

​						$$f_0(x) = \underbrace{arg\; min}_{c}\sum\limits_{i=1}^{m}L(y_i, c)$$

(2) 对 t= 1,2,3,...,T:

​	(a) 对i = 1,2,3,...,m 计算负梯度：

​						$$r_{ti} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]_{f(x) = f_{t-1}(x)}$$

​	(b)利用$(x_i,r_{ti})\;\; (i=1,2,..m)$,我们可以拟合一颗CART回归树，得到了第$t$颗回归树，其对应的叶节点区域$R_{tj}, j =1,2,..., J$。其中J为叶子节点的个数。

​	(c) 对叶子区域 $j =1,2,..J$计算最佳拟合值:

​						$$c_{tj} = \underbrace{arg min}_{c}\sum\limits_{x_i \in R_{tj}} L(y_i,f_{t-1}(x_i) +c)$$

​	(d) 更新强学习器

​						$$f_{t}(x) = f_{t-1}(x) + \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$$

(3) 得到强学习器f(x)的表达式:

​						$$f(x) = f_T(x) =f_0(x) + \sum\limits_{t=1}^{T}\sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$$

### 二元分类

对于二元GBDT，如果用类似于逻辑回归的**对数似然损失函数**，则损失函数为：

​					$$L(y, f(x)) = log(1+ exp(-yf(x)))$$

其中$y \in\{-1, +1\}$。则此时的负梯度误差为：

​					$$r_{ti} = -\bigg[\frac{\partial L(y, f(x_i)))}{\partial f(x_i)}\bigg]_{f(x) = f_{t-1}(x)} = y_i/(1+exp(y_if(x_i)))$$

对于生成的决策树，我们各个叶子节点的最佳残差拟合值为：

​					$$c_{tj} = \underbrace{arg\; min}_{c}\sum\limits_{x_i \in R_{tj}} log(1+exp(-y_i(f_{t-1}(x_i) +c)))$$

由于上式比较难优化，我们一般使用近似值代替：

​					$$c_{tj} =\sum\limits_{x_i \in R_{tj}}r_{ti}\bigg /\sum\limits_{x_i \in R_{tj}}|r_{ti}|(1-|r_{ti}|)$$



### 多元分类

多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为：

​					$$L(y, f(x)) = - \sum\limits_{k=1}^{K}y_klog\;p_k(x)$$

其中如果样本输出类别为k，则$y_k=1$。第k类的概率$p_k(x)$的表达式为：

​					$$p_k(x) = exp(f_k(x)) \bigg / \sum\limits_{l=1}^{K} exp(f_l(x))$$

集合上两式，我们可以计算出第$t$轮的第$i$个样本对应类别$l$的负梯度误差为：

​					$$r_{til} =-\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]_{f_k(x) = f_{l, t-1} (x)} = y_{il} - p_{l, t-1}(x_i)$$

观察上式可以看出，其实这里的误差就是样本$i$对应类别$l$的真实概率和$t-1$轮预测概率的差值。

对于生成的决策树，我们各个叶子节点的最佳残差拟合值为：

​					$$c_{tjl} = \underbrace{arg \min}_{c_{jl}}\sum\limits_{i=0}^{m}\sum\limits_{k=1}^{K} L(y_k, f_{t-1, l}(x) + \sum\limits_{j=0}^{J}c_{jl} I(x_i \in R_{tj}))$$

由于上式比较难优化，我们一般使用近似值代替：

​					$$c_{tjl} = \frac{K-1}{K}  \frac{\sum\limits_{x_i \in R_{tjl}}r_{til}}{\sum\limits_{x_i \in R_{til}}|r_{til}|(1-|r_{til}|)}$$

## GBDT正则化

除了和Adaboost算法一样的正则化方法，GBDT还有第二种正则化方法：通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。

使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。

## GBDT常用损失函数

### 分类

分类算法的损失函数一般有两类：对数损失函数和指数损失函数

- 对数损失函数：二元$$L(y, f(x)) = log(1+ exp(-yf(x)))$$（sigmod函数），多元：$$L(y, f(x)) = - \sum\limits_{k=1}^{K}y_klog p_k(x)$$，其中$$p_k(x) = exp(f_k(x)) \bigg / \sum\limits_{l=1}^{K} exp(f_l(x))$$（softmax函数）
- 指数损失函数：$$L(y, f(x)) = exp(-yf(x))$$

### 回归

- 均方差：$$L(y, f(x)) =(y-f(x))^2$$

- 绝对损失：$$L(y, f(x)) =|y-f(x)|$$，对应负梯度误差为：$$sign(y_i-f(x_i))$$

- Huber损失：它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。损失函数如下：

  ​						$$L(y, f(x))=\begin{cases}\frac{1}{2}(y-f(x))^2 , {|y-f(x)| \leq \delta}\\ \delta(|y-f(x)| - \frac{\delta}{2}),{|y-f(x)|  \delta} \end{cases}$$

- 分位数损失：。它对应的是分位数回归的损失函数，表达式为:

  ​						$$L(y, f(x)) =\sum\limits_{y \geq f(x)}\theta|y - f(x)| + \sum\limits_{y (x)}(1-\theta)|y - f(x)|$$

  其中$\theta$为分位数，需要我们在回归前指定。

> 对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。

## GBDT优缺点

GBDT主要的优点有：

- 可以灵活处理各种类型的数据，包括连续值和离散值。
- 在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。
- 使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。

GBDT的主要缺点有：

- 由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。