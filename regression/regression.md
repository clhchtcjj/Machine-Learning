# 回归分析

> 回归是用于估计两种变量之间关系的统计过程。当用于分析变量和一个多变量之间的关系时，该算法能够提供很多建模和分析多变量的技巧。具体一点说，回国分析可以帮助我们理解任意一个自变量变化，另一个自变量不变时，因变量变化的典型值。最常见的是，回归分析能在给定自变量的条件下估计出因变量的条件期望。

### 普通最小二乘回归（Ordinary Least Squares Regression，OLSR）

> 最小二乘（又称最小平方法）是一种数学优化技术。它通过**最小化误差的平方和**寻找数据的最佳函数匹配。利用最小二乘法可以简便的求得未知数据，并使得这些求得的数据与实际数据之间的误差的平方和为最小。最小二乘法还可以用于曲线拟合。

均方误差有很好的几何意义，它对应了常用的欧式距离。

普通最小二乘回归，又称为线性二乘回归。下面是模型的推导：







更一般的情况是：多元线性回归，即样本由$d$个属性描述。下面是推导过程：



**潜在问题：**

> 在许多任务中，我们会遇到大量的变量，其数目甚至超过了样例数（$d>m$）,那么$X^TX$显然不满秩。此时可以求解出多个$\hat{\vec{w}}$，它们都可以使得均方误差最小化。选择哪一个解作为输出，将由学习算法的归纳偏好决定，常见的做法是引入正则化项。

**重点：**

1. **自变量与因变量之间必须要有线性关系；**
2. 多重共线性、自相关和异方差对多元线性回归的影响很大；
3. 线性回归对异常值非常敏感，其能严重影响回归线，最终影响预测值；
4. 在多元的自变量中，我们可以通过前进法、后退法和逐步法去选择在于显著的自变量。

**补充：**

多重共线性：多重共线性（Multicollinearity）是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确[百度百科]。通俗的说，就是变量之间有较强的相关性，影响模型的预测能力。解决多重共线问题可以考虑一下几种方法：

1. 直接删除

2. 采用逐步回归法：逐步回归的基本思想是将变量逐个引入模型，每引入一个解释变量后都要进行F检验，并对已经选入的解释变量逐个进行t检验，当原来引入的解释变量由于后面解释变量的引入变得不再显著时，则将其删除。以确保每次引入新的变量之前回归方程中只包含显著性变量。这是一个反复的过程，直到既没有显著的解释变量选入回归方程，也没有不显著的解释变量从回归方程中剔除为止。以保证最后所得到的解释变量集是最优的。

   依据上述思想，可利用逐步回归筛选并剔除引起多重共线性的变量，其具体步骤如下：先用被解释变量对每一个所考虑的解释变量做简单回归，然后以对被解释变量贡献最大的解释变量所对应的回归方程为基础，再逐步引入其余解释变量。经过逐步回归，使得最后保留在模型中的解释变量既是重要的，又没有严重多重共线性。

3. 改变变量的表现形式：对变量进行非线性变化取log等，可以有效的降低变量之间的相关性

4. 增加样本数量

5. 引入正则化项

6. 进行主成分分析：通过主成分分析提取主要特征，从而忽略次要的成分，得到相关性很低的特征

   ​

## 逻辑回归

当因变量是二分类时（0/1、yes/no、true/false）时我们应该使用逻辑回归。



**重点：**

1. 在二分类问题中使用的非常多；
2. 逻辑回归因其应用非线性log转换方法，使得其不需要自变量与因变量之间有线性关系；
3. 为了方式过拟合和欠拟合，我们应该确保每个变量都是显著的。应该使用逐步回归方法去估计逻辑回归；
4. 要求没有共线性；
5. 如果因变量是序数型的，则称为序数型逻辑回归；
6. 如果因变量有多个，则称为多项逻辑回归。



## 多项式回归

自变量的**指数**超过1，则称为多项式回归。

在这个回归技术中，最适合的不是一条直线，而是一条曲线。



**重点**

1. 在很多情况下，我们为了降低误差，会使用多项式回归，但这样会造成过拟合。
2. 解决上述问题的一个行之有效的方法是：数据可视化。即观测数据与模型的拟合程度。特别是要看曲线的结尾部分，看它的形状和趋势是否有意义。高的数据项往往会产生特别罕见的预测值。



## 逐步回归

当我们要处理多个自变量时，我们就需要这个回归方法。在这个方法中选择变量是通过自动过程实现的，不需要人工干预。

在这个过程中，是通过观察统计值，比如判定系数，t值和最小信息准则去筛选变量。逐步回归变量一般是基于特定的标准加入或移除变量来拟合回归模型。一些常用的逐步回归方法如下:

1. 标准逐步回归做两件事：只要是需要每一步它都会添加或移除一些变量；
2. 后退法是开始于所有变量，然后逐渐移除一些不显著的变量；
3. 前进法是开始于最显著的变量，然后在模型中逐渐增加次显著变量；
4. 这个模型技术的目的是为了用最少的变量去最大化模型的预测能力，也是一种降维技术。